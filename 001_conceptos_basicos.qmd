---
title: Inferencia Bayesiana
subtitle: Conceptos b谩sicos
format: html
---

## Introducci贸n

La inferencia bayesiana es un enfoque para la estad铆stica en el que las probabilidades representan un grado de creencia, o estados subjetivos de incertidumbre. Esta perspectiva es diferente de la frecuentista, donde las probabilidades representan frecuencias de eventos en largas series de repeticiones.

La esencia de la inferencia bayesiana se resume en el Teorema de Bayes:

$$
P(A|B) = \frac{P(B|A)\times P(A)}{P(B)} 
$$

Donde:

- $P(A|B)$ es la probabilidad posterior de $A$ dado $B$
- $P(B|A)$ es la probabilidad de $B$ dado $A$ (verosimilitud)
- $P(A)$ es la probabilidad de $A$ (prior)
- $P(B)$ es la probabilidad marginal de $B$

En el contexto de la inferencia bayesiana:

- **Prior (Priori)**: Representa nuestra creencia inicial sobre un par谩metro antes de observar cualquier dato. Puede ser subjetivo o basarse en informaci贸n previa.
- **Likelihood (Verosimilitud)**: Es c贸mo los datos informan sobre el par谩metro de inter茅s.
- **Posterior (Posteriori)**: Es la actualizaci贸n de nuestra creencia sobre el par谩metro despu茅s de observar los datos. Es una combinaci贸n del prior y la verosimilitud.
- **Evidence (Evidencia)**: Es una constante de normalizaci贸n. En la pr谩ctica, a menudo no es necesario calcularla directamente, ya que estamos interesados en el posterior relativo.

::: {.callout-tip}
## Ejemplo Simple: Lanzamiento de una Moneda 

Supongamos que tienes una moneda y quieres saber si est谩 trucada. Lanzas la moneda varias veces y cuentas el n煤mero de caras. Usando la inferencia bayesiana, puedes actualizar tu creencia sobre la probabilidad de que salga cara bas谩ndote en los datos observados.

**Prior**: Puedes empezar creyendo que la moneda es justa, por lo que la probabilidad de cara es 0,5. Sin embargo, para ser menos restrictivo, puedes usar una distribuci贸n beta como prior, que es una distribuci贸n continua entre 0 y 1. Si eliges la Beta(1,1), es equivalente a un prior uniforme, es decir, todas las probabilidades de cara son igualmente posibles.

**Likelihood**: Si lanzas la moneda $n$ veces y obtienes $k$ caras, la verosimilitud se describe con una distribuci贸n binomial.

**Posterior**: Usando el teorema de Bayes, combinas el prior y la verosimilitud para obtener una distribuci贸n posterior para la probabilidad de cara.

:::

### Ejemplo en R

**Supuestos**:
1. **Prior**: Utilizaremos una distribuci贸n Beta(1,1) como prior, lo que indica que inicialmente creemos que todas las probabilidades de obtener cara son igualmente posibles.
2. **Likelihood**: Supongamos que lanzamos la moneda 100 veces y obtenemos 60 caras. Usaremos una distribuci贸n binomial para modelar esto.
3. **Posterior**: La distribuci贸n posterior es proporcional al producto del prior y la verosimilitud. Afortunadamente, con la elecci贸n de una distribuci贸n Beta como prior y una distribuci贸n binomial como verosimilitud, el posterior tambi茅n es una distribuci贸n Beta. Esta es una de las propiedades 煤tiles de la distribuci贸n Beta: es el prior conjugado para la distribuci贸n binomial.

```{r}
# Instala y carga las bibliotecas necesarias (si a煤n no lo has hecho)
# install.packages("ggplot2")
library(ggplot2)

# Par谩metros
n <- 100  # n煤mero total de lanzamientos
k <- 60   # n煤mero de caras obtenidas

# Par谩metros para la distribuci贸n prior Beta(1,1)
alpha_prior <- 1
beta_prior <- 1

# Par谩metros para la distribuci贸n posterior Beta(k+alpha, n-k+beta)
alpha_posterior <- k + alpha_prior
beta_posterior <- n - k + beta_prior

# Generamos datos para visualizaci贸n
p <- seq(0, 1, by = 0.01)
prior <- dbeta(p, shape1 = alpha_prior, shape2 = beta_prior)
likelihood <- dbinom(k, size = n, prob = p)
posterior <- dbeta(p, shape1 = alpha_posterior, shape2 = beta_posterior)

# Creamos un data frame para la visualizaci贸n
df <- data.frame(p = p, 
                 prior = prior, 
                 likelihood = likelihood / max(likelihood),  # Normalizamos para la visualizaci贸n
                 posterior = posterior)

# Visualizamos las distribuciones
ggplot(df, aes(x = p)) +
  geom_line(aes(y = prior, color = "Priori"), lwd = 1.5) +
  geom_line(aes(y = likelihood, color = "Verosimilitud (escalada)"), lwd = 1.5, linetype = "dashed") +
  geom_line(aes(y = posterior, color = "Posteriori"), lwd = 1.5) +
  labs(title = "Inferencia Bayesiana: Lanzamiento de una Moneda",
       y = "Densidad", x = "Probabilidad de caras",
       color = "") +
  theme_minimal()

```



## Ventajas de la inferencia Beyesiana

1. **Incorporaci贸n de Conocimiento Previo**: Una de las principales ventajas es la capacidad de incorporar conocimientos previos (o creencias) sobre un problema en el an谩lisis. Esto es especialmente 煤til en situaciones donde los datos son escasos.

::: {.callout-tip}
 Imagina que est谩s lanzando una moneda, pero antes de realizar cualquier lanzamiento, un experto te dice que cree que la moneda est谩 ligeramente sesgada hacia caras. Puedes incorporar esta creencia usando un prior que se incline hacia caras, por ejemplo, una distribuci贸n Beta(3,2).

```{r}
# Simulaci贸n en R
set.seed(123)
prior <- rbeta(1000, 3, 2)
hist(prior, main="Prior Distribution", xlab="Probability of Heads", col="skyblue", border="white")
```

:::

2. **Estimaciones Probabil铆sticas**: En lugar de hacer estimaciones puntuales de par谩metros, la inferencia bayesiana proporciona distribuciones completas (posteriores), lo que nos da una idea de la incertidumbre asociada con nuestras estimaciones.

::: {.callout-tip}
 En un estudio cl铆nico, se quiere determinar la eficacia de un nuevo medicamento. En lugar de simplemente decir que el medicamento tiene un X% de eficacia, con la inferencia bayesiana puedes decir que hay un 95% de probabilidad de que la eficacia est茅 entre Y% y Z%.

```{r}
# Simulaci贸n en R
# Supongamos que el prior es Beta(10, 10) y que observamos 70 茅xitos y 30 fallos con el medicamento.
posterior <- rbeta(1000, 10 + 70, 10 + 30)
hist(posterior, main="Posterior Distribution", xlab="Efficacy", col="lightgreen", border="white")
quantile(posterior, c(0.025, 0.975))
```

:::

3. **Modelado Jer谩rquico**: Permite modelar la estructura jer谩rquica en datos, donde los par谩metros pueden tener sus propios priors que dependen de otros par谩metros.

::: {.callout-tip}
Supongamos que est谩s estudiando el rendimiento de los estudiantes en diferentes escuelas. En lugar de tratar cada escuela como independiente, puedes usar un modelo jer谩rquico para compartir informaci贸n entre escuelas, creyendo que hay una distribuci贸n subyacente com煤n para todas las escuelas.

```{r}
# Supongamos que tienes datos sobre puntajes de ex谩menes de estudiantes de 10 escuelas.
# Puedes modelar el puntaje promedio de cada escuela como proveniente de una distribuci贸n normal global.

# Esto es solo un esquema y no un c贸digo ejecutable:
# model {
#   for (i in 1:10) {
#     school_mean[i] ~ dnorm(global_mean, global_sd)
#     score[i] ~ dnorm(school_mean[i], school_sd[i])
#   }
#   global_mean ~ dnorm(0, 100)
#   global_sd ~ dunif(0, 100)
# }

```

:::

4. **Flexibilidad**: La inferencia bayesiana es flexible en t茅rminos de las distribuciones que se pueden usar y c贸mo se pueden combinar.


::: {.callout-warning}
## Desaf铆os y Consideraciones
1. **Elecci贸n del Prior**: La elecci贸n del prior puede ser subjetiva y, en algunos casos, puede influir significativamente en el posterior, especialmente cuando hay pocos datos.

2. **Coste Computacional**: Algunos modelos bayesianos pueden ser computacionalmente intensivos, especialmente cuando se utilizan m茅todos de muestreo como Markov Chain Monte Carlo (MCMC) para obtener estimaciones del posterior.

3. **Convergencia**: Cuando se utilizan t茅cnicas de muestreo, es esencial asegurarse de que las cadenas hayan convergido a la distribuci贸n objetivo. Esto puede requerir herramientas y diagn贸sticos espec铆ficos.
:::